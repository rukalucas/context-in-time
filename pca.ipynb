{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis demo notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tasks import InstructedTimingTask, SequenceInstructedTask\n",
    "from models.rnn import RNN\n",
    "from analysis import (\n",
    "    generate_data,\n",
    "    do_pca,\n",
    "    visualize_pca,\n",
    "    plot_cross_period_variance,\n",
    "    animate_pca,\n",
    ")\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Model and Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'logs/transition_to_inferred_sharp/checkpoint_step_2000.pt'\n",
    "\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# task = InstructedTimingTask(w_m=0.05, input_noise_std=0.05)\n",
    "task = SequenceInstructedTask(w_m=0.05, input_noise_std=0.05, trials_per_sequence=40)\n",
    "\n",
    "model = RNN(\n",
    "    hidden_size=256,\n",
    "    noise_std=0.0  # No noise during evaluation\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from step {checkpoint['step']}\")\n",
    "print(f\"Model: {model.hidden_size} hidden units\")\n",
    "print(f\"Task: {task.trials_per_sequence} trials per sequence\")\n",
    "\n",
    "# Generate data with reward feedback\n",
    "print(\"Generating data...\")\n",
    "data_dict = generate_data(task, model, num_trials=80)\n",
    "rules = np.array([data_dict['batch'][i]['metadata']['rule'][0] for i in range(len(data_dict['batch']))])\n",
    "print(f\"Rule 1 count: {(rules == 1).sum()}, Rule 2 count: {(rules == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random trial\n",
    "batch = data_dict['batch']\n",
    "outputs = data_dict['outputs']\n",
    "trial_idx = np.random.randint(0, len(batch)) # sample random trial\n",
    "# trial_idx = 3\n",
    "trial_outputs = outputs[trial_idx].numpy()\n",
    "trial_inputs = batch[trial_idx]['inputs'][0].numpy()\n",
    "trial_targets = batch[trial_idx]['targets'][0].numpy()\n",
    "trial_eval_mask = batch[trial_idx]['eval_mask'][0].numpy()\n",
    "trial_loss_mask = batch[trial_idx]['loss_mask'][0].numpy()\n",
    "\n",
    "# Create figure using task method\n",
    "fig = task.create_trial_figure(\n",
    "    inputs=trial_inputs,\n",
    "    outputs=trial_outputs,\n",
    "    targets=trial_targets,\n",
    "    eval_mask=trial_eval_mask,\n",
    "    trial_idx=trial_idx,\n",
    "    batch=batch,\n",
    "    batch_idx=0,\n",
    "    loss_mask=trial_loss_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do PCA. Combine projections, event segments, periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on full trajectories\n",
    "result_full = do_pca(\n",
    "    data_dict,\n",
    "    task,\n",
    "    # periods=['timing','decision'],\n",
    "    # projection=['output_dim_0'],\n",
    "    model=model,\n",
    "    n_components=3\n",
    ")\n",
    "\n",
    "segments = [\n",
    "    {'start': 'timing_start', 'end': 'first_pulse', 'alpha': 0.2, 'label': 'Pre-first pulse'},\n",
    "    {'start': 'first_pulse', 'end': 'decision_start', 'alpha': 1.0, 'label': 'Interval timing'},\n",
    "    {'start': 'decision_start', 'end': 'trial_end', 'alpha': 0.8, 'label': 'Decision'},\n",
    "]\n",
    "\n",
    "print(f\"PCA data shape: {result_full['pca_data'].shape}\")\n",
    "print(f\"Axis: {result_full['axis_labels']}\")\n",
    "\n",
    "plot = visualize_pca(\n",
    "    result_full,\n",
    "    plot_3d=False,\n",
    "    # segments=segments,\n",
    "    num_trials=40,\n",
    "    color_by='rule',\n",
    ")\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# HTML(plot.to_html5_video()) # replace visualize_pca by animate_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Period Variance\n",
    "\n",
    "How well do PCs from one period explain variance in other periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_cross_period_variance(\n",
    "    data_dict,\n",
    "    task=task,\n",
    "    n_components=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psychometric Function\n",
    "\n",
    "P(Anti saccade) vs interval duration for each rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import compute_psychometric_curves\n",
    "\n",
    "# Compute psychometric curves for both rules\n",
    "results = compute_psychometric_curves(\n",
    "    task,\n",
    "    model,\n",
    "    num_trials_per_interval=100,\n",
    "    rules=[1, -1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "for rule in [1, -1]:\n",
    "    rule_label = 'C1' if rule == 1 else 'C2'\n",
    "    anti_probs = results['anti_probs'][rule]\n",
    "    ax.plot(results['intervals'], anti_probs, \n",
    "            marker='o', label=f'{rule_label} rule', linewidth=2)\n",
    "\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(task.decision_threshold, color='gray', linestyle='--', alpha=0.5, \n",
    "           label=f'Threshold ({task.decision_threshold}ms)')\n",
    "ax.set_xlabel('Interval (ms)')\n",
    "ax.set_ylabel('P(Anti saccade)')\n",
    "ax.set_title('Psychometric Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Explained\n",
    "\n",
    "How many principal components are needed to capture neural variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with many components to analyze dimensionality\n",
    "result_dims = do_pca(\n",
    "    data_dict,\n",
    "    task,\n",
    "    periods='all',\n",
    "    n_components=20\n",
    ")\n",
    "\n",
    "# Extract variance explained\n",
    "var_explained = result_dims['explained_variance']\n",
    "var_explained_pct = 100 * var_explained / var_explained.sum()\n",
    "cumulative_var = np.cumsum(var_explained_pct)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot of variance explained\n",
    "ax = axes[0]\n",
    "x = np.arange(1, len(var_explained_pct) + 1)\n",
    "ax.bar(x, var_explained_pct, alpha=0.7)\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Variance Explained (%)')\n",
    "ax.set_title('Variance Explained by Each PC')\n",
    "ax.set_xticks(x[::2])  # Show every other tick\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance plot\n",
    "ax = axes[1]\n",
    "ax.plot(x, cumulative_var, marker='o', linewidth=2, markersize=4)\n",
    "ax.axhline(90, color='red', linestyle='--', alpha=0.5, label='90% variance')\n",
    "ax.axhline(95, color='orange', linestyle='--', alpha=0.5, label='95% variance')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Variance Explained (%)')\n",
    "ax.set_title('Cumulative Variance Explained')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "n_90 = np.argmax(cumulative_var >= 90) + 1\n",
    "n_95 = np.argmax(cumulative_var >= 95) + 1\n",
    "print(f\"Components needed for 90% variance: {n_90}\")\n",
    "print(f\"Components needed for 95% variance: {n_95}\")\n",
    "print(f\"\\nTop 5 PCs explain: {cumulative_var[4]:.1f}% of variance\")\n",
    "print(f\"Top 10 PCs explain: {cumulative_var[9]:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: clear all plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
